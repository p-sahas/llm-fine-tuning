{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774ffaea",
   "metadata": {},
   "source": [
    "#### 00. Install dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480afda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Core training libraries\n",
    "!pip install -q \\\n",
    "    transformers==4.44.2 \\\n",
    "    datasets==2.20.0 \\\n",
    "    tokenizers==0.19.1 \\\n",
    "    accelerate==0.34.2 \\\n",
    "    peft==0.12.0 \\\n",
    "    trl==0.9.6 \\\n",
    "    bitsandbytes==0.43.1 \\\n",
    "    evaluate==0.4.2\n",
    "\n",
    "# Utilities\n",
    "!pip install -q \\\n",
    "    numpy \\\n",
    "    pandas \\\n",
    "    scikit-learn \\\n",
    "    rich \\\n",
    "    pyyaml \\\n",
    "    python-dotenv \\\n",
    "    tqdm\n",
    "\n",
    "# Evaluation (requires pydantic v2)\n",
    "!pip install -q --upgrade pydantic\n",
    "!pip install -q google-genai rouge-score\n",
    "\n",
    "print(\" Installation complete!\")\n",
    "print(\" All dependencies compatible (pydantic v2 + google-genai)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432dec47",
   "metadata": {},
   "source": [
    "## 1. Setting Up Environment Variables (Secrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a3c9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create .env file with API key\n",
    "import os\n",
    "\n",
    "# Write .env file\n",
    "# with open('.env', 'w') as f:\n",
    "#     # Add the secrets if needed\n",
    "#     f.write('GOOGLE_API_KEY=<api_key_here>\\n')\n",
    "#     f.write('HF_TOKEN=<api_key_here>\\n')\n",
    "\n",
    "# print(\" .env file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab4c6cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .env file not found\n"
     ]
    }
   ],
   "source": [
    "# Verify it's loaded\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Show only key names for security\n",
    "try:\n",
    "    with open('.env', 'r') as f:\n",
    "        print(\" Keys in .env file:\")\n",
    "        print(\"=\"*60)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                key = line.split('=')[0]\n",
    "                value_preview = line.split('=')[1][:10] + \"...\" if '=' in line else \"\"\n",
    "                print(f\"  {key} = {value_preview}\")\n",
    "        print(\"=\"*60)\n",
    "except FileNotFoundError:\n",
    "    print(\" .env file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6032fc",
   "metadata": {},
   "source": [
    "## 2. Environment & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37cfb258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT CHECK\n",
      "============================================================\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n",
      " WARNING: CUDA not available. Training will be VERY slow on CPU.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\" WARNING: CUDA not available. Training will be VERY slow on CPU.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f29630",
   "metadata": {},
   "source": [
    "## 3. Seeds & Determinism\n",
    "\n",
    "Setting up random seeds for reproducibility. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8a137",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4138758937.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Set seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Set environment variable for Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Set seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Note: These settings may impact performance\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\" Seeds set to {SEED} for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ba553",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Login\n",
    "\n",
    "If you want to push your finetuned adapter to the Hugging Face Hub, uncomment and run the login line below.\n",
    "\n",
    "Hugging Face token with write permissions. Get one at: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n",
    "!hf auth login --token $HF_TOKEN\n",
    "\n",
    "print(\"â„¹ Hugging Face login skipped. Uncomment login() to push models to Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e475e96",
   "metadata": {},
   "source": [
    "## 5. Configuration (Single Source of Truth)\n",
    "\n",
    "All hyperparameters and settings in one place. **Edit here** to customize your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca4ce9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONFIGURATION (COLAB FREE TIER)\n",
      "============================================================\n",
      "{'base_model': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
      " 'bnb_4bit_compute_dtype': torch.float16,\n",
      " 'bnb_4bit_quant_type': 'nf4',\n",
      " 'bnb_4bit_use_double_quant': True,\n",
      " 'dataset_name': 'lavita/AlpaCare-MedInstruct-52k',\n",
      " 'dataset_split': 'train',\n",
      " 'dataset_subsample': 500,\n",
      " 'do_sample': True,\n",
      " 'eval_steps': 100,\n",
      " 'gradient_accumulation_steps': 64,\n",
      " 'hf_username': '<your-username>',\n",
      " 'hub_model_name': 'zuucrew-medical-assistant',\n",
      " 'learning_rate': 2e-05,\n",
      " 'load_in_4bit': True,\n",
      " 'logging_steps': 10,\n",
      " 'lora_alpha': 32,\n",
      " 'lora_dropout': 0.05,\n",
      " 'lora_r': 16,\n",
      " 'lora_target_modules': ['q_proj',\n",
      "                         'k_proj',\n",
      "                         'v_proj',\n",
      "                         'o_proj',\n",
      "                         'gate_proj',\n",
      "                         'up_proj',\n",
      "                         'down_proj'],\n",
      " 'max_length': 512,\n",
      " 'max_new_tokens': 128,\n",
      " 'max_steps': 250,\n",
      " 'num_train_epochs': 1,\n",
      " 'output_dir': 'outputs/adapter',\n",
      " 'per_device_train_batch_size': 1,\n",
      " 'push_to_hub': False,\n",
      " 'save_steps': 200,\n",
      " 'save_total_limit': 2,\n",
      " 'temperature': 0.0,\n",
      " 'train_val_split': 0.9,\n",
      " 'warmup_ratio': 0.03}\n",
      "============================================================\n",
      "Compute dtype: torch.float16\n",
      "Using BF16: False\n",
      "Effective batch size: 64\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "# Auto-detect compute dtype (BF16 requires compute capability >= 8.0)\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"base_model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    # Alternative for tighter VRAM: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    # For GGUF export, prefer: \"meta-llama/Llama-3.2-3B-Instruct\" or Mistral models\n",
    "\n",
    "    # Dataset\n",
    "    \"dataset_name\": \"lavita/AlpaCare-MedInstruct-52k\",\n",
    "    \"dataset_split\": \"train\",\n",
    "    \"dataset_subsample\": 500,  # Colab-safe: 500 | Local: 1500\n",
    "    \"train_val_split\": 0.9,  # 90% train, 10% validation\n",
    "\n",
    "    # Tokenization\n",
    "    \"max_length\": 512,  # Colab: 512 | Local: 1024\n",
    "\n",
    "    # Training\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": 250,  # Colab: 250 | Local: 600\n",
    "    \"per_device_train_batch_size\": 1,  # Colab: 1 | Local: 2\n",
    "    \"gradient_accumulation_steps\": 64,  # Colab: 64 | Local: 32\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 200,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_total_limit\": 2,\n",
    "\n",
    "    # LoRA\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "\n",
    "    # Quantization\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": compute_dtype,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "\n",
    "    # Output\n",
    "    \"output_dir\": \"outputs/adapter\",\n",
    "    \"push_to_hub\": False,\n",
    "\n",
    "    # Generation\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.0,  # Deterministic\n",
    "    \"do_sample\": True,\n",
    "\n",
    "    # HF credentials\n",
    "    'hf_username': '<your-username>',\n",
    "    'hub_model_name': 'zuucrew-medical-assistant',\n",
    "}\n",
    "\n",
    "# Effective batch size\n",
    "effective_batch_size = CONFIG[\"per_device_train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIGURATION (COLAB FREE TIER)\")\n",
    "print(\"=\"*60)\n",
    "pprint(CONFIG)\n",
    "print(\"=\"*60)\n",
    "print(f\"Compute dtype: {compute_dtype}\")\n",
    "print(f\"Using BF16: {use_bf16}\")\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496429db",
   "metadata": {},
   "source": [
    "## 6. Dataset Loader (+ Fallback)\n",
    "\n",
    "Load the medical instruction dataset, map fields robustly, and create train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc7b123d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-591287516.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_medical_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Load dataset with robust field mapping and fallback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.20.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "\n",
    "def load_medical_dataset(dataset_name, split, subsample, seed=42):\n",
    "    \"\"\"Load dataset with robust field mapping and fallback.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Try loading from Hugging Face\n",
    "        print(f\" Loading dataset: {dataset_name}...\")\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "        dataset = dataset.shuffle(seed=seed).select(range(min(subsample, len(dataset))))\n",
    "        print(f\" Loaded {len(dataset)} examples from Hugging Face\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load from Hugging Face: {e}\")\n",
    "        print(\" Creating synthetic fallback dataset...\")\n",
    "\n",
    "        # Create synthetic medical instruction data\n",
    "        synthetic_data = []\n",
    "        templates = [\n",
    "            {\n",
    "                \"instruction\": \"Explain the following medical term in simple language.\",\n",
    "                \"input\": \"Hypertension\",\n",
    "                \"output\": \"Hypertension, commonly known as high blood pressure, is a condition where the force of blood against artery walls is consistently too high. This can lead to serious health complications if left untreated.\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"What are the common symptoms of the following condition?\",\n",
    "                \"input\": \"Type 2 Diabetes\",\n",
    "                \"output\": \"Common symptoms of Type 2 Diabetes include increased thirst, frequent urination, increased hunger, fatigue, blurred vision, slow-healing sores, and frequent infections.\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Provide general advice for managing the following health issue.\",\n",
    "                \"input\": \"Chronic back pain\",\n",
    "                \"output\": \"Managing chronic back pain typically involves: maintaining good posture, regular low-impact exercise like swimming or walking, maintaining a healthy weight, using proper lifting techniques, and consulting with healthcare providers for appropriate treatment options.\"\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Duplicate to reach ~120 examples\n",
    "        for i in range(40):\n",
    "            for template in templates:\n",
    "                synthetic_data.append(template)\n",
    "\n",
    "        # Save to temporary JSONL\n",
    "        with open(\"/tmp/synthetic_medical.jsonl\", \"w\") as f:\n",
    "            for item in synthetic_data[:subsample]:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "        dataset = load_dataset(\"json\", data_files=\"/tmp/synthetic_medical.jsonl\", split=\"train\")\n",
    "        print(f\" Created synthetic dataset with {len(dataset)} examples\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def map_dataset_fields(example):\n",
    "    \"\"\"Robustly map dataset fields to instruction/input/output schema.\"\"\"\n",
    "\n",
    "    # Try to find instruction\n",
    "    instruction = None\n",
    "    for key in [\"instruction\", \"question\", \"prompt\", \"task\"]:\n",
    "        if key in example and example[key]:\n",
    "            instruction = str(example[key]).strip()\n",
    "            break\n",
    "\n",
    "    # Try to find input (optional)\n",
    "    input_text = \"\"\n",
    "    for key in [\"input\", \"context\", \"passage\", \"history\"]:\n",
    "        if key in example and example[key]:\n",
    "            input_text = str(example[key]).strip()\n",
    "            break\n",
    "\n",
    "    # Try to find output/target\n",
    "    output = None\n",
    "    for key in [\"output\", \"response\", \"answer\", \"target\", \"completion\"]:\n",
    "        if key in example and example[key]:\n",
    "            output = str(example[key]).strip()\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": input_text,\n",
    "        \"output\": output\n",
    "    }\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_medical_dataset(\n",
    "    CONFIG[\"dataset_name\"],\n",
    "    CONFIG[\"dataset_split\"],\n",
    "    CONFIG[\"dataset_subsample\"],\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"\\n Dataset before cleaning: {len(dataset)} examples\")\n",
    "\n",
    "# Map fields\n",
    "dataset = dataset.map(map_dataset_fields)\n",
    "\n",
    "# Drop rows with missing instruction or output\n",
    "dataset = dataset.filter(lambda x: x[\"instruction\"] is not None and x[\"output\"] is not None)\n",
    "\n",
    "print(f\" Dataset after cleaning: {len(dataset)} examples\")\n",
    "print(f\" Dropped {CONFIG['dataset_subsample'] - len(dataset)} examples with missing data\\n\")\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = dataset.train_test_split(\n",
    "    train_size=CONFIG[\"train_val_split\"],\n",
    "    seed=SEED\n",
    ")\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\" Train: {len(train_dataset)} | Validation: {len(val_dataset)}\")\n",
    "print(\"\\n Sample example:\")\n",
    "print(train_dataset[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
