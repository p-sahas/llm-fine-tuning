{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774ffaea",
   "metadata": {},
   "source": [
    "#### 00. Install dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480afda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Core training libraries\n",
    "!pip install -q \\\n",
    "    transformers==4.44.2 \\\n",
    "    datasets==2.20.0 \\\n",
    "    tokenizers==0.19.1 \\\n",
    "    accelerate==0.34.2 \\\n",
    "    peft==0.12.0 \\\n",
    "    trl==0.9.6 \\\n",
    "    bitsandbytes==0.43.1 \\\n",
    "    evaluate==0.4.2\n",
    "\n",
    "# Utilities\n",
    "!pip install -q \\\n",
    "    numpy \\\n",
    "    pandas \\\n",
    "    scikit-learn \\\n",
    "    rich \\\n",
    "    pyyaml \\\n",
    "    python-dotenv \\\n",
    "    tqdm\n",
    "\n",
    "# Evaluation (requires pydantic v2)\n",
    "!pip install -q --upgrade pydantic\n",
    "!pip install -q google-genai rouge-score\n",
    "\n",
    "print(\" Installation complete!\")\n",
    "print(\" All dependencies compatible (pydantic v2 + google-genai)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432dec47",
   "metadata": {},
   "source": [
    "## 1. Setting Up Environment Variables (Secrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a3c9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create .env file with API key\n",
    "import os\n",
    "\n",
    "# Write .env file\n",
    "# with open('.env', 'w') as f:\n",
    "#     # Add the secrets if needed\n",
    "#     f.write('GOOGLE_API_KEY=<api_key_here>\\n')\n",
    "#     f.write('HF_TOKEN=<api_key_here>\\n')\n",
    "\n",
    "# print(\" .env file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab4c6cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .env file not found\n"
     ]
    }
   ],
   "source": [
    "# Verify it's loaded\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Show only key names for security\n",
    "try:\n",
    "    with open('.env', 'r') as f:\n",
    "        print(\" Keys in .env file:\")\n",
    "        print(\"=\"*60)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                key = line.split('=')[0]\n",
    "                value_preview = line.split('=')[1][:10] + \"...\" if '=' in line else \"\"\n",
    "                print(f\"  {key} = {value_preview}\")\n",
    "        print(\"=\"*60)\n",
    "except FileNotFoundError:\n",
    "    print(\" .env file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6032fc",
   "metadata": {},
   "source": [
    "## 2. Environment & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37cfb258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT CHECK\n",
      "============================================================\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n",
      " WARNING: CUDA not available. Training will be VERY slow on CPU.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\" WARNING: CUDA not available. Training will be VERY slow on CPU.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f29630",
   "metadata": {},
   "source": [
    "## 3. Seeds & Determinism\n",
    "\n",
    "Setting up random seeds for reproducibility. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8a137",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4138758937.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Set seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Set environment variable for Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Set seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Note: These settings may impact performance\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\" Seeds set to {SEED} for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ba553",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Login\n",
    "\n",
    "If you want to push your finetuned adapter to the Hugging Face Hub, uncomment and run the login line below.\n",
    "\n",
    "Hugging Face token with write permissions. Get one at: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n",
    "!hf auth login --token $HF_TOKEN\n",
    "\n",
    "print(\"â„¹ Hugging Face login skipped. Uncomment login() to push models to Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e475e96",
   "metadata": {},
   "source": [
    "## 5. Configuration (Single Source of Truth)\n",
    "\n",
    "All hyperparameters and settings in one place. **Edit here** to customize your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca4ce9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONFIGURATION (COLAB FREE TIER)\n",
      "============================================================\n",
      "{'base_model': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
      " 'bnb_4bit_compute_dtype': torch.float16,\n",
      " 'bnb_4bit_quant_type': 'nf4',\n",
      " 'bnb_4bit_use_double_quant': True,\n",
      " 'dataset_name': 'lavita/AlpaCare-MedInstruct-52k',\n",
      " 'dataset_split': 'train',\n",
      " 'dataset_subsample': 500,\n",
      " 'do_sample': True,\n",
      " 'eval_steps': 100,\n",
      " 'gradient_accumulation_steps': 64,\n",
      " 'hf_username': '<your-username>',\n",
      " 'hub_model_name': 'zuucrew-medical-assistant',\n",
      " 'learning_rate': 2e-05,\n",
      " 'load_in_4bit': True,\n",
      " 'logging_steps': 10,\n",
      " 'lora_alpha': 32,\n",
      " 'lora_dropout': 0.05,\n",
      " 'lora_r': 16,\n",
      " 'lora_target_modules': ['q_proj',\n",
      "                         'k_proj',\n",
      "                         'v_proj',\n",
      "                         'o_proj',\n",
      "                         'gate_proj',\n",
      "                         'up_proj',\n",
      "                         'down_proj'],\n",
      " 'max_length': 512,\n",
      " 'max_new_tokens': 128,\n",
      " 'max_steps': 250,\n",
      " 'num_train_epochs': 1,\n",
      " 'output_dir': 'outputs/adapter',\n",
      " 'per_device_train_batch_size': 1,\n",
      " 'push_to_hub': False,\n",
      " 'save_steps': 200,\n",
      " 'save_total_limit': 2,\n",
      " 'temperature': 0.0,\n",
      " 'train_val_split': 0.9,\n",
      " 'warmup_ratio': 0.03}\n",
      "============================================================\n",
      "Compute dtype: torch.float16\n",
      "Using BF16: False\n",
      "Effective batch size: 64\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "# Auto-detect compute dtype (BF16 requires compute capability >= 8.0)\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"base_model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    # Alternative for tighter VRAM: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    # For GGUF export, prefer: \"meta-llama/Llama-3.2-3B-Instruct\" or Mistral models\n",
    "\n",
    "    # Dataset\n",
    "    \"dataset_name\": \"lavita/AlpaCare-MedInstruct-52k\",\n",
    "    \"dataset_split\": \"train\",\n",
    "    \"dataset_subsample\": 500,  # Colab-safe: 500 | Local: 1500\n",
    "    \"train_val_split\": 0.9,  # 90% train, 10% validation\n",
    "\n",
    "    # Tokenization\n",
    "    \"max_length\": 512,  # Colab: 512 | Local: 1024\n",
    "\n",
    "    # Training\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": 250,  # Colab: 250 | Local: 600\n",
    "    \"per_device_train_batch_size\": 1,  # Colab: 1 | Local: 2\n",
    "    \"gradient_accumulation_steps\": 64,  # Colab: 64 | Local: 32\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 200,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_total_limit\": 2,\n",
    "\n",
    "    # LoRA\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "\n",
    "    # Quantization\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": compute_dtype,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "\n",
    "    # Output\n",
    "    \"output_dir\": \"outputs/adapter\",\n",
    "    \"push_to_hub\": False,\n",
    "\n",
    "    # Generation\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.0,  # Deterministic\n",
    "    \"do_sample\": True,\n",
    "\n",
    "    # HF credentials\n",
    "    'hf_username': '<your-username>',\n",
    "    'hub_model_name': 'zuucrew-medical-assistant',\n",
    "}\n",
    "\n",
    "# Effective batch size\n",
    "effective_batch_size = CONFIG[\"per_device_train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIGURATION (COLAB FREE TIER)\")\n",
    "print(\"=\"*60)\n",
    "pprint(CONFIG)\n",
    "print(\"=\"*60)\n",
    "print(f\"Compute dtype: {compute_dtype}\")\n",
    "print(f\"Using BF16: {use_bf16}\")\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
